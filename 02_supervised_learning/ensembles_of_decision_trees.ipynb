{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import mglearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles of Decision Trees\n",
    "- *Ensembles* are methods that combine multiple machine learning models to create more powerful models (this can be applied to a variety of models)\n",
    "Two ensemble models have proven to be effective on a wide range od datasets for classification and regression, both of which use decision trees as their building blocks: \n",
    "1. Random Forests\n",
    "2. Gradient Boosted Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "- main drawback of decision trees is that they tend to overfit the training data\n",
    "- random forests, essentially a collection of decision trees, where each tree is slightly different from the others.\n",
    "    -  each tree will overfit slightly differently over its part of the data\n",
    "    - averaging their results makes the model more robust and generalizable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Building random forests\n",
    "- number of decision trees (*n_estimators*)\n",
    "- bootstrap sample of data (random sample w/ replacement), missing will be approximately 1/3 of the data while other data points will be repeated\n",
    "- we then build a decision tree based upon this newly created dataset\n",
    "- at each node, we take a subset of the features and perform a best test, rather than over the whole feature set. The number of features we select is controlled by the *max_features* parameter\n",
    "- a high *max_features* means that the trees will be quite similar, and fit the data easdily using the most distinctive features. \n",
    "- a low *max_features* means that the trees in the RF will be quite different, and each tree might need to be very deep in order to fit the data well.\n",
    "\n",
    "1. The prediction using RF, the algorithm first makes a prediction for every tree in the forest. We then average these results to get a final prediction for regression. For classification, a \"soft voting\" strategy is used, provided the probability of the output label from each tree the probabilities by all the trees are averaged, and the class with the highest probability is predicted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Analyzing random forests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X,y = make_moons(n_samples=100, noise=0.25, random_state=3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
